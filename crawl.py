from playwright.sync_api import sync_playwright
from datetime import datetime
import os

class MSNNewsScraper:
    def __init__(self):
        self.browser = None

    def get_article_links(self, page, max_links=10):
        for _ in range(5):
            page.mouse.wheel(0, 3000)
            page.wait_for_timeout(1000)

        container = page.query_selector("#articles-container")
        if not container:
            print("â— #articles-containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        cards = container.query_selector_all(":scope > clf-ca-card")
        links = []

        for card in cards:
            try:
                shadow = card.evaluate_handle("e => e.shadowRoot")

                cs_card = shadow.query_selector("cs-card cs-content-card")
                if cs_card:
                    href = cs_card.get_attribute("href")
                    if href and href.startswith("http"):
                        links.append(href)

                if len(links) >= max_links:
                    break
            except Exception as e:
                print(f"âš ï¸ ì¹´ë“œ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                continue

        return links


    def scrape_article(self, page, url):
        page.goto(url)
        page.wait_for_timeout(3000)

        title_el = page.query_selector("h1")
        body_el = page.query_selector(".article-body")

        if not title_el or not body_el:
            return None

        title = title_el.inner_text().strip()
        body = body_el.inner_text().strip()

        if not body:
            return None

        return f"# {title}\n\n{body}"

    def save_article(self, content, url):
        if not os.path.exists("articles"):
            os.makedirs("articles")
        filename = f"articles/article_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ì›ë³¸ URL: {url}\n\n")
            f.write(content)
        return filename

    def run(self, topic_url):
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=False,
                args=["--window-position=0,10000"]
            )
            page = browser.new_page()
            page.goto(topic_url)
            page.wait_for_timeout(5000)

            print("[ğŸ”] ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì¤‘...")
            links = self.get_article_links(page)
            print(f"[âœ…] {len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ë¨.")

            for idx, link in enumerate(links, 1):
                print(f"\n[{idx}] ìŠ¤í¬ë˜í•‘ ì¤‘: {link}")
                try:
                    content = self.scrape_article(page, link)
                    if content:
                        filename = self.save_article(content, link)
                        print(f"  â¤ ì €ì¥ ì™„ë£Œ: {filename}")
                    else:
                        print("  â¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                except Exception as e:
                    print(f"  â¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

            browser.close()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    urls = {
        "ê²½ì œí•™": "https://www.msn.com/ko-kr/channel/topic/%EA%B2%BD%EC%A0%9C%ED%95%99/tp-Y_55a61254-2d9d-4a2a-813f-8197f063dda3?ocid=msedgntp",
        "ì •ì¹˜": "https://www.msn.com/ko-kr/channel/topic/%EC%A0%95%EC%B9%98/tp-Y_6aa79722-759d-4dbc-af04-abaabe57a18f?ocid=msedgntp",
        "ì—°ì˜ˆì¸": "https://www.msn.com/ko-kr/channel/topic/%EC%97%B0%EC%98%88%EC%9D%B8/tp-Y_94abd02a-491e-4628-abc7-389d81057107?ocid=msedgntp",
        "ìŠ¤í¬ì¸ ": "https://www.msn.com/ko-kr/channel/topic/%EC%8A%A4%ED%8F%AC%EC%B8%A0/tp-Y_bc40ffcd-5e18-475c-8752-cb7ca85085a9?ocid=msedgntp",
    }
    for subject, topic_url in urls.items():
        print(f"{subject} í¬ë¡¤ë§ ì¤‘...")
        scraper = MSNNewsScraper()
        scraper.run(topic_url)
