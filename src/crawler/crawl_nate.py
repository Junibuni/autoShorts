import os
import requests
from bs4 import BeautifulSoup

class NATENewsScraper:
    def __init__(self, today, article_path="articles"):
        self.article_path = os.path.join(article_path, "nate")
        
        if not os.path.exists(self.article_path):
            os.makedirs(self.article_path)
            
        today_path = os.path.join(self.article_path, today)
        if not os.path.exists(today_path):
            os.makedirs(today_path)
    
    def get_article_links(self, page):
        headers = {
            'User-Agent': 'Mozilla/5.0'
        }

        try:
            response = requests.get(page, headers=headers)
            if response.status_code != 200:
                print(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            blocks = soup.select('#newsContents > div > div.postRankSubjectList.f_clear > div')

            links = []
            for block in blocks:
                a_tag = block.select_one('div > a')
                if a_tag and 'href' in a_tag.attrs:
                    links.append(f"https:{a_tag['href']}")

            rank_links = soup.select('#postRankSubject > ul > li > a')
            for a_tag in rank_links:
                if a_tag and 'href' in a_tag.attrs:
                    links.append(f"https:{a_tag['href']}")
                
            return links
        
        except Exception as e:
            print(f"ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def scrape_article(self, page, url):
        pass
    
    def save_article(self, content, url, subject):
        topic_path = os.path.join(self.article_path, self.today, subject)

    
    def run(self, subject, topic_url):
        print("[ğŸ”] ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì¤‘...")
        links = self.get_article_links(topic_url)
        print(f"[âœ…] {len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ë¨.")
        print(links)
        # idx = 1
        # for link in links:
        #     if idx > self.max_links:
        #         print(f"[âœ…] {idx-1}ê°œ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ.")
        #         break
        #     print(f"\n[{idx}] ìŠ¤í¬ë˜í•‘ ì¤‘: {link}")
        #     try:
        #         content = self.scrape_article(page, link)
        #         if content:
        #             filename = self.save_article(content, link, subject)
        #             print(f"  â¤ ì €ì¥ ì™„ë£Œ: {filename}")
        #             idx += 1
        #         else:
        #             print("  â¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        #     except Exception as e:
        #         print(f"  â¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

def crawl_news(today, max_links=20):
    scraper = NATENewsScraper(
        today
    )
    
    from src.urls import URLS_NATE
    for subject, topic_url in URLS_NATE.items():
        if not topic_url:
            continue
        print(f"\n\n{subject} í¬ë¡¤ë§ ì¤‘...")
        scraper.run(subject, topic_url)