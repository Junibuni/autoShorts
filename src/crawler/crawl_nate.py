import os
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
class NATENewsScraper:
    def __init__(self, today, max_links, article_path="articles"):
        self.article_path = os.path.join(article_path, "nate")
        self.max_links = max_links
        self.today = today
        
        if not os.path.exists(self.article_path):
            os.makedirs(self.article_path)
            
        today_path = os.path.join(self.article_path, today)
        if not os.path.exists(today_path):
            os.makedirs(today_path)
    
    def get_article_links(self, page):
        headers = {
            'User-Agent': 'Mozilla/5.0'
        }

        try:
            response = requests.get(page, headers=headers)
            if response.status_code != 200:
                print(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            blocks = soup.select('#newsContents > div > div.postRankSubjectList.f_clear > div')

            links = []
            for block in blocks:
                a_tag = block.select_one('div > a')
                if a_tag and 'href' in a_tag.attrs:
                    links.append(f"https:{a_tag['href']}")

            rank_links = soup.select('#postRankSubject > ul > li > a')
            for a_tag in rank_links:
                if a_tag and 'href' in a_tag.attrs:
                    links.append(f"https:{a_tag['href']}")
                
            return links
        
        except Exception as e:
            print(f"ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def scrape_article(self, page, url):
        headers = {
            'User-Agent': 'Mozilla/5.0'
        }

        try:
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                print(f"ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            title_tag = soup.select_one('#articleView > h1') or soup.select_one('#cntArea > h1')
            title = title_tag.get_text(strip=True) if title_tag else "(ì œëª© ì—†ìŒ)"

            # ë³¸ë¬¸ ì„ íƒ: ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„ (ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ ì‚¬ìš©)
            content_tag = None
            for selector in ['#realArtcContents', '#articleContetns > div']:
                candidate = soup.select_one(selector)
                if candidate:
                    content_tag = candidate
                    break

            if not content_tag:
                print("  â¤ ë³¸ë¬¸ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None

            for a in content_tag.find_all('a', href=True):
                a.decompose()

            body = content_tag.get_text(separator='\n', strip=True)
            
            pattern = r"[â–¶â˜â–²â—‡â– â—†]"
            body = re.sub(pattern, "", body)
            
            lines = body.splitlines()
            non_empty_lines = [line for line in lines if line.strip()]
            body = '\n'.join(non_empty_lines)

            if len(body) < 400:
                print("  â¤ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ì„œ ì œì™¸ë¨")
                return None

            return f"# {title}\n\n{body}"

        except Exception as e:
            print(f"ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def save_article(self, content, url, subject, idx):
        topic_path = os.path.join(self.article_path, self.today, subject)
        
        if not os.path.exists(topic_path):
            os.makedirs(topic_path)
            
        filename = os.path.join(topic_path, f"article_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{idx}.md")
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ì›ë³¸ URL: {url}\n\n")
            f.write(content)
        return filename

    
    def run(self, subject, topic_url):
        print("[ğŸ”] ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì¤‘...")
        links = self.get_article_links(topic_url)
        print(f"[âœ…] {len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ë¨.")
        idx = 1
        for link in links:
            if idx > self.max_links:
                print(f"[âœ…] {idx-1}ê°œ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ.")
                break
            print(f"\n[{idx}] ìŠ¤í¬ë˜í•‘ ì¤‘: {link}")
            try:
                content = self.scrape_article(topic_url, link)
                if content:
                    filename = self.save_article(content, link, subject, idx)
                    print(f"  â¤ ì €ì¥ ì™„ë£Œ: {filename}")
                    idx += 1
                else:
                    print("  â¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
            except Exception as e:
                print(f"  â¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

def crawl_news(today, max_links=20, save_path='articles'):
    scraper = NATENewsScraper(
        today,
        max_links,
        article_path=save_path
    )
    
    from src.urls import URLS_NATE
    for subject, topic_url in URLS_NATE.items():
        if not topic_url:
            continue
        print(f"\n\n{subject} í¬ë¡¤ë§ ì¤‘...")
        scraper.run(subject, topic_url)