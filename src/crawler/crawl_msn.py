from playwright.sync_api import sync_playwright
from datetime import datetime
import os
import re

PROMPT_TEMPLATE = """
ë„ˆëŠ” ìœ íŠœë¸Œ ì‡¼ì¸  ì œì‘ì„ ìœ„í•œ ì½˜í…ì¸  íë ˆì´í„°ì•¼. ë‚´ê°€ ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ì œê³µí•  í…Œë‹ˆ, ê·¸ ì¤‘ **ê°€ì¥ ìê·¹ì ì´ê³  ì‹œì²­ìì˜ í˜¸ê¸°ì‹¬ì„ ê°•í•˜ê²Œ ìê·¹í•  ìˆ˜ ìˆëŠ” {}ê°œì˜ ë‰´ìŠ¤ ì œëª©**ë§Œ ê³¨ë¼ì¤˜. ê°€ì¥ ìê·¹ì ì¸ ë‰´ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ì •ë¦¬í•´.

ë‹¤ìŒ ê¸°ì¤€ì„ ì ìš©í•´ì„œ íŒë‹¨í•´:
1. ê°ì • ìê·¹ ê°•ë„ê°€ ë†’ì€ê°€? (ë¶ˆì•ˆ, ë¶„ë…¸, ë†€ëŒ, ìœ„ê¸°ê° ë“±)
2. ê¶ê¸ˆì¦ì„ ìœ ë°œí•˜ëŠ” ë¯¸ì™„ì„±ëœ ì •ë³´ë‚˜ ë°˜ì „ì´ ìˆëŠ”ê°€?
3. ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“± ì‹œì²­ìì˜ ì‹¤ìƒí™œì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ë¯¼ê°í•œ ì´ìŠˆì¸ê°€?
4. ìœ íŠœë¸Œ ì‡¼ì¸  í¬ë§·(15ì´ˆ ë‚´ì™¸, ë¹ ë¥¸ ì „ê°œ)ì— ì–´ìš¸ë¦´ ë§Œí¼ ì„íŒ©íŠ¸ê°€ ìˆëŠ”ê°€?

â— ì•„ë˜ í˜•ì‹ì˜ **ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥**í•´. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ. ì •í™•íˆ ì•„ë˜ì²˜ëŸ¼ ì¶œë ¥í•´:

"[ì œëª© ID] ì œëª©1",
"[ì œëª© ID] ì œëª©2",
"[ì œëª© ID] ì œëª©3",
...

ì•„ë˜ëŠ” ë‰´ìŠ¤ ì œëª© ëª©ë¡ì´ì•¼:

{}
"""
class MSNNewsScraper:
    def __init__(self, today, max_links, openai_client, article_path="articles"):
        self.today = today
        self.max_links = max_links
        self.openai_client = openai_client
        self.article_path = os.path.join(article_path, "msn")
        
        if not os.path.exists(self.article_path):
            os.makedirs(self.article_path)
            
        today_path = os.path.join(self.article_path, today)
        if not os.path.exists(today_path):
            os.makedirs(today_path)

    def get_article_links(self, page):
        for _ in range(5):
            page.mouse.wheel(0, 3000)
            page.wait_for_timeout(1000)

        container = page.query_selector("#articles-container")
        if not container:
            print("â— #articles-containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        cards = container.query_selector_all(":scope > clf-ca-card")
        links = []
        titles = []
        
        news_idx = 0

        for card in cards:
            try:
                shadow = card.evaluate_handle("e => e.shadowRoot")

                cs_card = shadow.query_selector("cs-card cs-content-card")
                if cs_card:
                    href = cs_card.get_attribute("href")
                    title = cs_card.get_attribute("title")
                    if href and href.startswith("http"):
                        titles.append(f"{news_idx}. {title}")
                        links.append(href)
                        news_idx += 1

                if len(links) >= 30:
                    break
            except Exception as e:
                print(f"âš ï¸ ì¹´ë“œ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                continue
        
        max_link_lim = 30 if self.max_links+5 > 30 else self.max_links+5
        titles_prompt = '\n'.join(titles)

        prompt = PROMPT_TEMPLATE.format(max_link_lim, titles_prompt)

        client = self.openai_client
        
        response = client.responses.create(
            model="o4-mini",
            instructions="You are a helpful assistant that strictly follows the prompt.",
            input=prompt
        )
        
        reply = response.output_text
        print(reply)
        link_indices = [int(re.search(r"\[(\d+)\]", line).group(1)) for line in reply.strip().splitlines() if re.search(r"\d", line)]

        return [links[i] for i in link_indices]


    def scrape_article(self, page, url):
        page.goto(url)
        page.wait_for_timeout(3000)

        title_el = page.query_selector("h1")
        body_el = page.query_selector(".article-body")

        if not title_el or not body_el:
            return None

        title = title_el.inner_text().strip()
        body = body_el.inner_text().strip()

        if not body:
            return None
        
        if len(body) < 400 or body.count('.') < 2:
            print("  â¤ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ì„œ ì œì™¸ë¨")
            return None

        return f"# {title}\n\n{body}"

    def save_article(self, content, url, subject):
        topic_path = os.path.join(self.article_path, self.today, subject)
        
        if not os.path.exists(topic_path):
            os.makedirs(topic_path)
            
        filename = os.path.join(topic_path, f"article_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ì›ë³¸ URL: {url}\n\n")
            f.write(content)
        return filename

    def run(self, subject, topic_url):
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=False,
                args=["--window-position=0,10000"]
            )
            page = browser.new_page()
            page.goto(topic_url)
            page.wait_for_timeout(5000)

            print("[ğŸ”] ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì¤‘...")
            links = self.get_article_links(page)
            print(f"[âœ…] {len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ë¨.")

            idx = 1
            for link in links:
                if idx > self.max_links:
                    print(f"[âœ…] {idx-1}ê°œ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ.")
                    break
                print(f"\n[{idx}] ìŠ¤í¬ë˜í•‘ ì¤‘: {link}")
                try:
                    content = self.scrape_article(page, link)
                    if content:
                        filename = self.save_article(content, link, subject)
                        print(f"  â¤ ì €ì¥ ì™„ë£Œ: {filename}")
                        idx += 1
                    else:
                        print("  â¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                except Exception as e:
                    print(f"  â¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

            browser.close()

def crawl_news(today, openai_client, max_links=20, save_path='articles'):
    scraper = MSNNewsScraper(
        openai_client=openai_client,
        today=today,
        max_links=max_links,
        article_path=save_path
    )
    
    from src.urls import URLS_MSN
    for subject, topic_url in URLS_MSN.items():
        if not topic_url:
            continue
        print(f"\n\n{subject} í¬ë¡¤ë§ ì¤‘...")
        scraper.run(subject, topic_url)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawl_news()
